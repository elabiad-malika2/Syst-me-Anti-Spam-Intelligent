{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "5a524f1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes : 10\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "df = pd.read_csv('../Data/processed/processed_data.csv')\n",
    "\n",
    "print(f\"Valeurs manquantes : {df['clean_text'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "99737cfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valeurs manquantes : 0\n"
     ]
    }
   ],
   "source": [
    "df = df.dropna(subset=['clean_text'])\n",
    "print(f\"Valeurs manquantes : {df['clean_text'].isnull().sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8c83878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taille du dataset chargé : (28801, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>label_text</th>\n",
       "      <th>clean_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>softwar understand oem softwar lead temptat fi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>perspect ferc regulatori action client conf ca...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>want tri ci li thought way expens viagra per d...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>ham</td>\n",
       "      <td>enron hpl actual decemb teco tap enron hpl ga ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>spam</td>\n",
       "      <td>look cheap high qualiti softwar rotat napoleon...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   label label_text                                         clean_text\n",
       "0      1       spam  softwar understand oem softwar lead temptat fi...\n",
       "1      0        ham  perspect ferc regulatori action client conf ca...\n",
       "2      1       spam  want tri ci li thought way expens viagra per d...\n",
       "3      0        ham  enron hpl actual decemb teco tap enron hpl ga ...\n",
       "4      1       spam  look cheap high qualiti softwar rotat napoleon..."
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Taille du dataset chargé : {df.shape}\")\n",
    "display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a37506b0",
   "metadata": {},
   "source": [
    "###  Séparation du dataset en données d'entraînement et de test\n",
    "\n",
    "Dans cette étape, nous séparons notre dataset en deux parties :\n",
    "\n",
    "- **80%** des données pour l'entraînement du modèle (`X_train`, `y_train`)\n",
    "- **20%** des données pour le test et l'évaluation (`X_test`, `y_test`)\n",
    "\n",
    "Cette séparation permet de mesurer correctement les performances du modèle sur des données qu’il n’a jamais vues.  \n",
    "Le paramètre `random_state=42` assure la reproductibilité : chaque exécution donnera le même split.\n",
    "\n",
    "Nous travaillons ici avec :\n",
    "- **X** : la colonne `clean_text` (les textes prétraités)\n",
    "- **y** : la colonne `label` (la classe associée à chaque texte)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "88cc8713",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training emails: 23040\n",
      "Testing emails : 5761\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Sélection des features (X) et de la variable cible (y)\n",
    "X = df['clean_text']     # Textes nettoyés\n",
    "y = df['label']          # Labels : 0 = ham, 1 = spam \n",
    "\n",
    "# Division du dataset : 80% entraînement, 20% test\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y,\n",
    "    test_size=0.2,        # 20% des données utilisées pour le test\n",
    "    random_state=42       # Assure que le split reste identique à chaque exécution\n",
    ")\n",
    "\n",
    "# Affichage des tailles des deux ensembles\n",
    "print(f\"Training emails: {X_train.shape[0]}\")\n",
    "print(f\"Testing emails : {X_test.shape[0]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aeeac12a",
   "metadata": {},
   "source": [
    "###  Vectorisation avec TF-IDF et sauvegarde du vectorizer\n",
    "\n",
    "Dans cette cellule nous transformons les textes en vecteurs numériques avec **TfidfVectorizer** et sauvegardons le vectorizer pour réutilisation future.\n",
    "\n",
    "**Étapes réalisées :**\n",
    "1. Création d'un dossier `../models` si nécessaire pour y stocker les artefacts (vectorizer, modèles, etc.).  \n",
    "2. Instanciation du `TfidfVectorizer` avec un maximum de **5000** features (`max_features=5000`) pour limiter la dimensionnalité.  \n",
    "3. **Fit** sur l'ensemble d'entraînement (`X_train`) pour apprendre le vocabulaire et transformer les textes en vecteurs TF-IDF.  \n",
    "4. **Transformation** des textes de test (`X_test`) en vecteurs TF-IDF en utilisant le vocabulaire appris.  \n",
    "5. Sauvegarde du vectorizer sur disque (`pickle`) dans `../saved_models/tfidf_vectorizer.pkl`.  \n",
    "\n",
    "**Pourquoi fit sur X_train seulement ?**  \n",
    "Pour éviter la fuite de données : le modèle ne doit jamais \"voir\" les données de test lors de l'apprentissage du vocabulaire.\n",
    "\n",
    "Les matrices `X_train_tfidf` et `X_test_tfidf` sont prêtes pour l’entraînement d’un modèle de classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a95cc0f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorization complete.\n",
      "Train Shape: (23040, 5000)\n",
      "Test Shape : (5761, 5000)\n",
      "Vectorizer saved to '../models/tfidf_vectorizer.pkl'\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# --- Créer le dossier 'saved_models' s'il n'existe pas ---\n",
    "os.makedirs('../models', exist_ok=True)\n",
    "\n",
    "# --- Initialisation du vectorizer ---\n",
    "tfidf = TfidfVectorizer(max_features=5000)  # limite à 5000 tokens les plus informatifs\n",
    "\n",
    "# --- Fit sur l'entraînement et transformation ---\n",
    "X_train_tfidf = tfidf.fit_transform(X_train)  # apprentissage du vocabulaire + transformation\n",
    "X_test_tfidf = tfidf.transform(X_test)        # transformation du test avec le vocabulaire appris\n",
    "\n",
    "# --- Sauvegarde du vectorizer pour réutilisation ---\n",
    "with open('../models/tfidf_vectorizer.pkl', 'wb') as f:\n",
    "    pickle.dump(tfidf, f)\n",
    "\n",
    "# --- Informations pour vérification ---\n",
    "print(\"Vectorization complete.\")\n",
    "print(f\"Train Shape: {X_train_tfidf.shape}\")  # exemple: (23048, 5000)\n",
    "print(f\"Test Shape : {X_test_tfidf.shape}\")   # exemple: (5763, 5000)\n",
    "print(\"Vectorizer saved to '../models/tfidf_vectorizer.pkl'\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d21141c",
   "metadata": {},
   "source": [
    "###  Sauvegarde des matrices TF-IDF et des labels\n",
    "\n",
    "Dans cette étape, nous sauvegardons les données transformées afin de pouvoir les réutiliser ultérieurement sans avoir à recalculer les vecteurs TF-IDF.  \n",
    "\n",
    "**Détails :**\n",
    "\n",
    "1. **Matrices TF-IDF (`X_train_tfidf` et `X_test_tfidf`)**  \n",
    "   - Ces matrices sont **sparse** (beaucoup de zéros) car chaque texte n'utilise qu'une partie du vocabulaire.  \n",
    "   - Nous les sauvegardons au format **`.npz`** grâce à `scipy.sparse.save_npz` :  \n",
    "     - Ce format est très efficace en espace mémoire.  \n",
    "     - Permet de recharger rapidement les matrices plus tard pour l’entraînement ou l’inférence.\n",
    "\n",
    "2. **Labels (`y_train` et `y_test`)**  \n",
    "   - Sauvegardés au format **CSV** pour faciliter la lecture et l’utilisation avec pandas.  \n",
    "\n",
    "**Structure des fichiers sauvegardés :**\n",
    "- `../data/processed/X_train_tfidf.npz` → matrice TF-IDF d'entraînement  \n",
    "- `../data/processed/X_test_tfidf.npz` → matrice TF-IDF de test  \n",
    "- `../data/processed/y_train.csv` → labels d'entraînement  \n",
    "- `../data/processed/y_test.csv` → labels de test  \n",
    "\n",
    "> Ces fichiers constituent un pipeline complet de données prétraitées et vectorisées, prêts pour l’entraînement d’un modèle de classification.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9cf80315",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrices TF-IDF et labels sauvegardés avec succès !\n",
      "X_train_tfidf -> ../data/processed/X_train_tfidf.npz\n",
      "X_test_tfidf  -> ../data/processed/X_test_tfidf.npz\n",
      "y_train       -> ../data/processed/y_train.csv\n",
      "y_test        -> ../data/processed/y_test.csv\n"
     ]
    }
   ],
   "source": [
    "import scipy.sparse\n",
    "import os\n",
    "\n",
    "# --- Créer le dossier 'processed' s'il n'existe pas ---\n",
    "os.makedirs('../data/processed', exist_ok=True)\n",
    "\n",
    "# --- Sauvegarde des matrices TF-IDF ---\n",
    "# Les matrices TF-IDF sont sparse (beaucoup de zéros)\n",
    "# Le format .npz est optimisé pour les matrices clairsemées et économise de l'espace disque\n",
    "scipy.sparse.save_npz('../data/processed/X_train_tfidf.npz', X_train_tfidf)\n",
    "scipy.sparse.save_npz('../data/processed/X_test_tfidf.npz', X_test_tfidf)\n",
    "\n",
    "# --- Sauvegarde des labels correspondants ---\n",
    "# Les labels sont sauvegardés au format CSV pour une lecture facile avec pandas\n",
    "y_train.to_csv('../data/processed/y_train.csv', index=False)\n",
    "y_test.to_csv('../data/processed/y_test.csv', index=False)\n",
    "\n",
    "# --- Informations de sortie ---\n",
    "print(\"Matrices TF-IDF et labels sauvegardés avec succès !\")\n",
    "print(\"X_train_tfidf -> ../data/processed/X_train_tfidf.npz\")\n",
    "print(\"X_test_tfidf  -> ../data/processed/X_test_tfidf.npz\")\n",
    "print(\"y_train       -> ../data/processed/y_train.csv\")\n",
    "print(\"y_test        -> ../data/processed/y_test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0b21cd5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
